import streamlit as st
from pipeline import load_documents, vectorize_documents, cluster_documents
from summarize import summarize_text_with_openai
from rag_module import build_retriever, ask_question
import pandas as pd
import os

# âœ… API í‚¤ ì²˜ë¦¬
try:
    api_key = st.secrets["OPENAI_API_KEY"]
except Exception:
    api_key = st.text_input("ğŸ”‘ OpenAI API Key", type="password")

st.set_page_config(page_title="ë¬¸ì„œ ë¶„ì„ + ì§ˆì˜ì‘ë‹µ", layout="wide")
st.title("ğŸ“„ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ë° ì§ˆì˜ì‘ë‹µ ëŒ€ì‹œë³´ë“œ")

# ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ
uploaded_files = st.file_uploader("ğŸ“‚ ë¶„ì„í•  ë¬¸ì„œ ì—…ë¡œë“œ", type=["txt", "pdf", "docx"], accept_multiple_files=True)

# ğŸ”˜ ìš”ì•½ ë°©ì‹
summary_method = st.selectbox("ìš”ì•½ ë°©ì‹ ì„ íƒ", ["ê¸°ë³¸ ìš”ì•½ (TF-IDF)", "OpenAI GPT ìš”ì•½"])

# ë¶„ì„ ì‹¤í–‰
if st.button("ë¬¸ì„œ ë¶„ì„ ì‹œì‘"):
    os.makedirs("documents", exist_ok=True)
    for file in uploaded_files:
        with open(os.path.join("documents", file.name), "wb") as f:
            f.write(file.getbuffer())

    docs = load_documents("documents")
    embeddings = vectorize_documents(docs)
    clustered_docs = cluster_documents(docs, embeddings, n_clusters=4)

    # ìš”ì•½ ì²˜ë¦¬
    for doc in clustered_docs:
        if summary_method == "OpenAI GPT ìš”ì•½" and api_key:
            try:
                doc["summary"] = summarize_text_with_openai(doc["text"], api_key)
            except Exception as e:
                doc["summary"] = f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}"
        else:
            doc.setdefault("summary", doc.get("summary", "ìš”ì•½ ì—†ìŒ"))

    st.success("âœ… ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ")
    df = pd.DataFrame(clustered_docs)
    st.session_state["docs"] = clustered_docs  # RAGìš© ì €ì¥

    # ê²°ê³¼ ì¶œë ¥
    for cluster_id in sorted(df["cluster"].unique()):
        st.header(f"ğŸ”¹ í´ëŸ¬ìŠ¤í„° {cluster_id}")
        for _, row in df[df["cluster"] == cluster_id].iterrows():
            st.subheader(f"ğŸ“„ {os.path.basename(row['file'])}")
            st.write(f"ğŸ“ ìš”ì•½: {row['summary']}")
            with st.expander("ğŸ“š ì „ì²´ í…ìŠ¤íŠ¸ ë³´ê¸°"):
                st.write(row["text"])

# ğŸ§  RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
st.markdown("---")
st.subheader("ğŸ’¬ ì—…ë¡œë“œëœ ë¬¸ì„œì— ì§ˆë¬¸í•´ë³´ì„¸ìš” (LangChain + OpenAI)")
question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if st.button("ì§ˆë¬¸ ì‹¤í–‰") and question:
    try:
        # retriever ìƒì„± í›„ QA ì‹¤í–‰
        retriever = build_retriever(st.session_state["docs"], api_key)
        response = ask_question(retriever, question, api_key)
        st.success("ğŸ§  GPT ì‘ë‹µ:")
        st.write(response)
    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ========================
# ğŸ“Š ì¶”ê°€ ê¸°ëŠ¥: í‚¤ì›Œë“œ DBí™”, í´ëŸ¬ìŠ¤í„°ë§, HTML ë¦¬í¬íŠ¸ ìƒì„±
# ========================
from ingest_documents import ingest_all_documents
from visualize_clusters import run_clustering
from generate_html_report import generate_html_report

st.markdown("---")
st.subheader("ğŸ“Š ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ë° ë¦¬í¬íŠ¸ ìë™í™”")

col1, col2, col3 = st.columns(3)

with col1:
    if st.button("ğŸ“‚ ë¬¸ì„œ DB ì €ì¥ (í‚¤ì›Œë“œ ì¶”ì¶œ)"):
        ingest_all_documents()
        st.success("âœ… ë¬¸ì„œ ë° í‚¤ì›Œë“œê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

with col2:
    if st.button("ğŸ§  ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"):
        run_clustering(output_csv="clustered_output.csv")
        st.success("âœ… KMeans í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ (CSV ì €ì¥ë¨)")

with col3:
    if st.button("ğŸ“ HTML ë¦¬í¬íŠ¸ ìƒì„±"):
        generate_html_report(csv_path="clustered_output.csv")
        st.success("âœ… HTML ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: cluster_report.html")

# í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
if os.path.exists("clustered_output.csv"):
    st.markdown("### ğŸ“„ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
    df = pd.read_csv("clustered_output.csv")
    st.dataframe(df[['filename', 'cluster']])
